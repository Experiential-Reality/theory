# neural.bld - Neural Networks Domain
#
# Status: VALIDATED (6.2% compensation effect)
#
# Architectures succeed when their BLD aligns with data's BLD.

structure NeuralNetworks

# =============================================================================
# BLD MAPPING
# =============================================================================
#
# | BLD | Neural | Examples |
# |-----|--------|----------|
# | B | Activation boundaries | ReLU at x=0, attention masks |
# | L | Connectivity | Dense (MLP), sparse (CNN), dynamic (Transformer) |
# | D | Dimensions | batch × layers × hidden |

B architecture: MLP | CNN | Transformer
  MLP -> dense_L, simple_B
  # Global L can compensate for simple B

  CNN -> sparse_local_L, pooling_B
  # Local L matches spatial locality
  # Cannot compensate for missing global features

  Transformer -> dynamic_L, attention_mask_B
  # Dynamic L adapts to input structure
  # Content-dependent connectivity

# =============================================================================
# COMPENSATION PRINCIPLE (VALIDATED)
# =============================================================================
#
# Global L can compensate for B deficiency, local L cannot.
#
# Why:
#   - B (activations) is topological: each neuron partitions locally
#   - L (connectivity) is geometric: dense spans globally, sparse stays local
#   - D×L accumulates: deep networks cascade soft → sharp
#   - D×B stays local: each neuron still makes local decision
#
# Validated formula:
#   Performance = a - b₁·ΔL - c·ΔB·ΔL
#
# Note: No independent ΔB term! B only matters when L also mismatched.
#
# Results:
#   - L effect: b₁ ≈ 0.025 per unit ΔL
#   - B×L interaction: c ≈ 0.061
#   - Diagonal advantage: 6.2% when structure matches

L connectivity: layer_n -> layer_n_plus_1 (pattern=dense|sparse|dynamic)

D batch: B [parallel]
D layers: L [sequential]
D hidden: H [parallel]

returns: NetworkAnalysis
